import pandas as pd
import cv2
import os
import argparse
from tqdm import tqdm
import paddle
from paddleocr import PaddleOCR
import logging
import numpy as np

# å±è”½æ— å…³æ—¥å¿—
logging.getLogger("ppocr").setLevel(logging.ERROR)


def preprocess_image(img):
    """
    å›¾åƒé¢„å¤„ç†ï¼šé’ˆå¯¹ä½åˆ†è¾¨ç‡å’Œæ¨¡ç³Šçš„å¹¿å‘Šç‰Œå›¾ç‰‡è¿›è¡Œå¢å¼º
    """
    # 1. å°ºå¯¸å¢å¼ºï¼šå¦‚æœå›¾ç‰‡é«˜åº¦è¿‡å°ï¼ˆå¸¸è§äºé•¿æ¡å¹…æˆªå›¾ï¼‰ï¼Œæ”¾å¤§ä»¥æå‡å°å­—è¯†åˆ«ç‡
    h, w = img.shape[:2]
    if h < 128:  # é˜ˆå€¼å¯è°ƒï¼Œé’ˆå¯¹é‚£äº›åªæœ‰30-50pxé«˜çš„é•¿æ¡å›¾
        scale_factor = 2.0
        img = cv2.resize(img, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_CUBIC)

    # 2. é”åŒ–å¤„ç†ï¼šå¢å¼ºè¾¹ç¼˜ï¼Œè§£å†³æ¨¡ç³Šå¯¼è‡´çš„å­—æ¯æ··æ·†ï¼ˆå¦‚ f/t, e/cï¼‰
    # ä½¿ç”¨æ ‡å‡†çš„é”åŒ–å·ç§¯æ ¸
    sharpen_kernel = np.array([
        [0, -1, 0],
        [-1, 5, -1],
        [0, -1, 0]
    ])
    img = cv2.filter2D(src=img, ddepth=-1, kernel=sharpen_kernel)

    return img


def parse_ocr_result(item):
    """
    å¥å£®çš„è§£æå‡½æ•°ï¼šå…¼å®¹å„ç§ PaddleOCR è¿”å›æ ¼å¼
    """
    text, score = "", 0.0
    try:
        # æƒ…å†µ 1: æˆ‘ä»¬åœ¨ sliding_window_ocr æ‰‹åŠ¨æ„é€ çš„ç®€å•å­—å…¸
        if isinstance(item, dict):
            text = item.get('text', '')
            score = item.get('score', 0.0)

            # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šå¦‚æœæ²¡æœ‰å–åˆ°ï¼Œå°è¯•æ—§é€»è¾‘
            if not text and 'rec_texts' in item:
                # è¿™ç§é€šå¸¸æ˜¯åŸå§‹å¤§å­—å…¸ï¼Œä¸åº”è¯¥èµ°åˆ°è¿™é‡Œï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                if len(item['rec_texts']) > 0:
                    text = item['rec_texts'][0]
                    score = item['rec_scores'][0]

        # æƒ…å†µ 2: æ ‡å‡† List/Tuple æ ¼å¼ [[bbox], (text, score)]
        elif isinstance(item, (list, tuple)):
            if len(item) >= 2:
                content = item[1]
                if isinstance(content, (list, tuple)) and len(content) >= 2:
                    text = content[0]
                    score = content[1]
                elif isinstance(content, str):
                    text = content
                    score = 1.0
    except Exception as e:
        print(f"Parse error: {e}")
        return "", 0.0

    return text, score


def sliding_window_ocr(ocr_engine, img_rgb):
    """
    æ»‘åŠ¨çª—å£åˆ‡ç‰‡è¯†åˆ«
    é’ˆå¯¹é•¿å®½æ¯”è¿‡å¤§çš„å›¾ç‰‡ï¼Œåˆ‡åˆ†æˆå¤šä¸ªé‡å çš„ç‰‡æ®µåˆ†åˆ«è¯†åˆ«ï¼Œæœ€åæ±‡æ€»ç»“æœã€‚
    è¿”å›æ ¼å¼ç»Ÿä¸€åŒ…è£…ä¸º [[result1, result2...]]
    """
    if img_rgb is None: return []
    h, w = img_rgb.shape[:2]
    if h == 0 or w == 0: return []

    aspect_ratio = w / float(h)

    # === [å‚æ•°è°ƒæ•´åŒº] ===
    # 1. åˆ‡ç‰‡è§¦å‘é˜ˆå€¼ï¼šé•¿å®½æ¯”è¶…è¿‡å¤šå°‘å¼€å§‹åˆ‡ç‰‡ï¼Ÿ(å»ºè®® 3.0)
    #    è°ƒä½æ­¤å€¼ä¼šè®©æ›´å¤šä¸­ç­‰é•¿åº¦çš„å›¾ç‰‡ä¹Ÿè¿›è¡Œåˆ‡ç‰‡ï¼Œæé«˜å¬å›ç‡ï¼Œä½†ä¼šé™ä½é€Ÿåº¦ã€‚
    SLICE_TRIGGER_RATIO = 3.0

    # 2. ç›®æ ‡åˆ‡ç‰‡æ¯”ä¾‹ï¼šå¸Œæœ›æ¯ä¸ªå°åˆ‡ç‰‡çš„é•¿å®½æ¯”æ˜¯å¤šå°‘ï¼Ÿ(å»ºè®® 3.0 - 4.0)
    #    è°ƒä½æ­¤å€¼ (å¦‚ 2.5) ä¼šäº§ç”Ÿæ›´å¤šã€æ›´çª„çš„åˆ‡ç‰‡ï¼Œå¯¹ä¸¥é‡å˜å½¢çš„é•¿å›¾æ•ˆæœæ›´å¥½ã€‚
    TARGET_SLICE_RATIO = 3.0

    # 3. é‡å ç‡ï¼šåˆ‡ç‰‡ä¹‹é—´çš„é‡å åŒºåŸŸæ¯”ä¾‹ (0.1 - 0.8, å»ºè®® 0.5)
    #    0.5 è¡¨ç¤ºé‡å ä¸€åŠã€‚é‡å è¶Šå¤šï¼Œè¾¹ç•Œå¤„çš„è¯è¶Šä¸å®¹æ˜“è¢«åˆ‡æ–­ï¼Œå»é‡é€»è¾‘è¶Šç¨³å¥ã€‚
    OVERLAP_RATIO = 0.5
    # ===================

    crops = []

    # å¦‚æœé•¿å®½æ¯”æœªè¾¾åˆ°è§¦å‘å€¼ï¼Œä¸åˆ‡ç‰‡ï¼Œç›´æ¥æ•´å›¾è¯†åˆ«
    if aspect_ratio < SLICE_TRIGGER_RATIO:
        crops.append(img_rgb)
    else:
        # åŠ¨æ€è®¡ç®—éœ€è¦åˆ‡å‡ ä»½
        # ä¾‹å¦‚ï¼šå›¾ç‰‡æ¯”ä¾‹ 10:1ï¼Œç›®æ ‡æ¯”ä¾‹ 3:1 -> åˆ‡ 4 ä»½
        num_slices = max(2, int(aspect_ratio / TARGET_SLICE_RATIO) + 1)

        step = w / num_slices
        overlap_width = step * OVERLAP_RATIO

        for i in range(num_slices):
            # è®¡ç®—åŒ…å«é‡å åŒºçš„åæ ‡
            start_x = max(0, int(i * step - overlap_width))
            end_x = min(w, int((i + 1) * step + overlap_width))

            if start_x >= end_x: continue

            crop = img_rgb[:, start_x:end_x]
            if crop.size > 0:
                crops.append(crop)

    results_pool = []

    for crop in crops:
        try:
            # è¿™é‡Œçš„ ocr è°ƒç”¨ä¿æŒåŸæ ·
            slice_res = ocr_engine.ocr(crop)
            if not slice_res: continue

            for res_item in slice_res:
                if not res_item: continue

                # é€‚é… Dict æ ¼å¼ (æ–°ç‰ˆ PaddleOCR)
                if isinstance(res_item, dict):
                    rec_texts = res_item.get('rec_texts', [])
                    rec_scores = res_item.get('rec_scores', [])
                    for t, s in zip(rec_texts, rec_scores):
                        results_pool.append({'text': t, 'score': s})

                # é€‚é… List æ ¼å¼ (æ—§ç‰ˆ PaddleOCR)
                elif isinstance(res_item, list):
                    results_pool.append(res_item)

        except Exception as e:
            print(f"âš ï¸ Error on slice: {e}")
            pass

    return [results_pool] if results_pool else []


def filter_contained_texts(df_group):
    """
    è¿‡æ»¤é€»è¾‘ï¼šåœ¨åŒä¸€ç§’å†…ï¼Œå¦‚æœçŸ­è¯æ˜¯é•¿è¯çš„å­ä¸²ï¼Œåˆ™ä¸¢å¼ƒçŸ­è¯ã€‚
    ä¾‹å¦‚ï¼šå­˜åœ¨ 'ä¹°ç†è´¢æ‰¾å¹³å®‰' å’Œ 'æ‰¾å¹³å®‰'ï¼Œåˆ™åˆ é™¤ 'æ‰¾å¹³å®‰'ã€‚
    """
    # 1. æŒ‰æ–‡æœ¬é•¿åº¦é™åºæ’åˆ—ï¼ˆä¼˜å…ˆä¿ç•™é•¿è¯ï¼‰
    # è¾…åŠ©åˆ—ï¼štext_len
    df_group['text_len'] = df_group['raw_text'].apply(len)
    sorted_df = df_group.sort_values(by='text_len', ascending=False)

    kept_indices = []
    kept_texts = []

    for idx, row in sorted_df.iterrows():
        current_text = row['raw_text']

        # æ£€æŸ¥å½“å‰è¯æ˜¯å¦æ˜¯ä»»ä½•â€œå·²ä¿ç•™è¯â€çš„å­ä¸²
        is_substring = False
        for kept in kept_texts:
            if current_text in kept:
                is_substring = True
                break

        # å¦‚æœä¸æ˜¯å­ä¸²ï¼Œæˆ–è€…æ˜¯å®Œå…¨ç›¸ç­‰çš„è¯ï¼ˆä½†å› ä¸ºæˆ‘ä»¬å‰é¢å·²ç»åšäº†åŒè¯å–æœ€é«˜åˆ†å»é‡ï¼Œè¿™é‡Œé€šå¸¸å¤„ç†çš„æ˜¯ä¸åŒè¯ï¼‰ï¼Œåˆ™ä¿ç•™
        if not is_substring:
            kept_indices.append(idx)
            kept_texts.append(current_text)

    return df_group.loc[kept_indices]


def is_garbage(text):
    """
    åƒåœ¾å­—ç¬¦è¿‡æ»¤å™¨
    """
    if not text: return True
    clean_text = text.strip()

    # 1. åŸºç¡€è¿‡æ»¤ï¼šå»é™¤é•¿åº¦å°äº2çš„çº¯æ•°å­—/å­—æ¯ (å¦‚ "A", "1")
    if len(clean_text) < 2:
        if clean_text.isascii() and clean_text.isalnum():
            return True

    # === [å¯é€‰ä¼˜åŒ–] ä¸¥æ ¼è¿‡æ»¤æ¨¡å¼ ===
    # éœ€æ±‚ï¼šæŠŠæ‰€æœ‰çº¯è‹±æ–‡ã€çº¯æ•°å­—ã€çº¯ç¬¦å·çš„å­—ç¬¦ä¸²ç›´æ¥åˆ å»ï¼Œåªä¿ç•™åŒ…å«ä¸­æ–‡çš„è¯ã€‚
    # é€»è¾‘ï¼šå¦‚æœæ•´ä¸ªå­—ç¬¦ä¸²éƒ½æ˜¯ ASCII å­—ç¬¦ (a-z, 0-9, @, ., etc.)ï¼Œåˆ™è§†ä¸ºåƒåœ¾ã€‚
    # æ“ä½œï¼šè‹¥è¦å¯ç”¨ï¼Œè¯·å–æ¶ˆä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Šã€‚

    # if clean_text.isascii():
    #     return True

    # ==============================

    return False


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--crops_dir', default='outputs/crops_more', help='Directory containing crop images')
    parser.add_argument('--output_csv', default='outputs/ad_logs/more_ad_result_test.csv', help='Final result csv')
    parser.add_argument('--sample_rate', type=int, default=2, help='Process 1 frame every N frames')
    args = parser.parse_args()

    metadata_path = os.path.join(args.crops_dir, 'metadata.csv')
    if not os.path.exists(metadata_path):
        print(f"âŒ Error: {metadata_path} not found.")
        return

    df = pd.read_csv(metadata_path)
    # æ ¹æ®é‡‡æ ·ç‡ç­›é€‰å¸§
    target_frames = df[df['frame_index'] % args.sample_rate == 0].copy()
    unique_frames = target_frames['frame_index'].unique()
    print(f"ğŸ“‚ Loaded {len(df)} records. Processing {len(target_frames)} crops.")
    print(f"ğŸ¯ Sampling Rate: {args.sample_rate}")
    print(f"âš¡ Processing {len(target_frames)} crops from {len(unique_frames)} unique frames.")

    # 1. è®¾ç½®å…¨å±€è®¾å¤‡
    if paddle.device.is_compiled_with_cuda():
        try:
            paddle.set_device('gpu')
            print("ğŸš€ [Step 2] PaddleOCR running on GPU")
        except:
            paddle.set_device('cpu')
    else:
        print("âš ï¸ PaddleOCR running on CPU")

    # 2. åˆå§‹åŒ–
    print("ğŸ“ Initializing PaddleOCR v3...")

    ocr = PaddleOCR(
        ocr_version='PP-OCRv4',
        use_textline_orientation=True,
        lang='ch',
        text_det_limit_side_len=12000,
        text_det_limit_type='max',
        # æ”¾å¤§æ£€æµ‹æ¡†ï¼Œè§£å†³è‰ºæœ¯å­—ç¬”ç”»åˆ†ç¦»é—®é¢˜
        text_det_unclip_ratio=1.8,
        # é™ä½æ£€æµ‹æ¡†é—¨æ§›ï¼Œæé«˜å¬å›ç‡
        text_det_box_thresh=0.25,
        # äºŒå€¼åŒ–é˜ˆå€¼
        text_det_thresh=0.15
    )

    results = []
    print("running OCR...")

    # 3. å¾ªç¯å¤„ç†
    for idx, row in tqdm(target_frames.iterrows(), total=len(target_frames)):
        img_path = row['crop_path']
        if not os.path.exists(img_path): continue

        try:
            img = cv2.imread(img_path)
            if img is None: continue

            img_processed = preprocess_image(img)
            if img_processed is None: continue

            img_rgb = cv2.cvtColor(img_processed, cv2.COLOR_BGR2RGB)

            # è°ƒç”¨ OCR
            ocr_res = sliding_window_ocr(ocr, img_rgb)

            # æ ¡éªŒç»“æœ
            if not ocr_res or not isinstance(ocr_res, list):
                continue

            flattened_res = ocr_res[0]
            if not flattened_res:
                continue

            for item in flattened_res:
                # è§£æç»“æœ
                text, score = parse_ocr_result(item)
                clean_text = str(text).strip()

                if score > 0.35 and len(clean_text) > 0:
                    if not is_garbage(clean_text):
                        results.append({
                            'second': row['second'],
                            'frame_index': row['frame_index'],
                            'raw_text': clean_text,
                            'score': score
                        })

        except Exception as e:
            print(f"Skipping error: {e}")
            pass

    # 4. ä¿å­˜ç»“æœä¸èšåˆé€»è¾‘
    if results:
        res_df = pd.DataFrame(results)

        # --- å¸§é—´å»é‡ï¼ˆåŒç§’ã€åŒè¯ï¼Œä¿ç•™æœ€é«˜åˆ†ï¼‰ ---
        # å…ˆæŒ‰åˆ†æ•°é™åºï¼Œè¿™æ · drop_duplicates é»˜è®¤ä¿ç•™ç¬¬ä¸€æ¡ï¼ˆæœ€é«˜åˆ†ï¼‰
        res_df = res_df.sort_values(by='score', ascending=False)
        res_df_dedup = res_df.drop_duplicates(subset=['second', 'raw_text'], keep='first').copy()

        # --- åŒ…å«å…³ç³»è¿‡æ»¤,è¿‡æ»¤å­ä¸²ï¼ˆåŒç§’å†…ï¼Œåˆ é™¤è¢«é•¿è¯åŒ…å«çš„çŸ­è¯ï¼‰ ---
        # ä½¿ç”¨ groupby å¯¹æ¯ä¸€ç§’çš„æ•°æ®åˆ†åˆ«åº”ç”¨ filter_contained_texts
        res_df_filtered = res_df_dedup.groupby('second', group_keys=False).apply(filter_contained_texts)

        # --- æ ¼å¼åŒ– ---
        res_df_filtered['formatted_text'] = res_df_filtered.apply(
            lambda x: f"{x['raw_text']}({x['score']:.2f})", axis=1
        )

        # --- èšåˆè¾“å‡º ---
        final_df = res_df_filtered.groupby('second')['formatted_text'].apply(
            lambda x: "; ".join(sorted(list(x)))
        ).reset_index()

        # é‡å‘½åå¹¶ä¿å­˜
        final_df.rename(columns={'formatted_text': 'text'}, inplace=True)

        os.makedirs(os.path.dirname(args.output_csv), exist_ok=True)
        final_df.to_csv(args.output_csv, index=False, encoding='utf-8-sig')
        print(f"\nâœ… Logs saved to {args.output_csv}")
        print(final_df.head(10))
    else:
        print("\nâš ï¸ No text detected in the selected frames.")


if __name__ == '__main__':
    main()
